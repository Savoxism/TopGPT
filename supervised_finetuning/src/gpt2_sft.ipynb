{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39aa5ee3",
   "metadata": {},
   "source": [
    "# TopGPT Instruction Fine-tuning\n",
    "This notebook fine-tunes a continually pre-trained GPT-2 model for instruction following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb52f705",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d09965",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install datasets transformers colorama peft bitsandbytes torch trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "HF_API_KEY = \"insert\"\n",
    "login(HF_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a500e30",
   "metadata": {},
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from colorama import Fore\n",
    "\n",
    "dataset = load_dataset(\"data\", split='train')\n",
    "print(Fore.YELLOW + str(dataset[2]) + Fore.RESET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c96c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "import torch\n",
    " \n",
    "def format_gpt2_instruction_template(batch, tokenizer):\n",
    "    \"\"\"\n",
    "    Format the dataset for GPT-2 instruction fine-tuning.\n",
    "    Uses a simple instruction-response format suitable for GPT-2.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # Access the inputs from the batch\n",
    "    questions = batch[\"question\"]\n",
    "    answers = batch[\"answer\"]\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        # GPT-2 instruction format\n",
    "        # Using special tokens to clearly separate instruction from response\n",
    "        text = f\"### Instruction:\\n{questions[i]}\\n\\n### Response:\\n{answers[i]}{tokenizer.eos_token}\"\n",
    "        samples.append(text)\n",
    "\n",
    "    return {\n",
    "        \"instruction\": questions,\n",
    "        \"response\": answers,\n",
    "        \"text\": samples  # The processed instruction-response text for each row\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b650331",
   "metadata": {},
   "source": [
    "# 3. Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551edafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"Savoxism/gpt2-large-continued-pretraining\"  # use the continually pretrained model \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model, \n",
    "    trust_remote_code=True,\n",
    "    token=HF_API_KEY,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = dataset.map(lambda x: format_gpt2_instruction_template(x, tokenizer), num_proc=8, batched=True, batch_size=10)\n",
    "print(Fore.LIGHTMAGENTA_EX + str(train_dataset[0]) + Fore.RESET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization config for efficient training\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"cuda:0\",\n",
    "    quantization_config=quant_config,\n",
    "    token=HF_API_KEY,  # replace with your Hugging Face token\n",
    "    cache_dir=\"./cache\",\n",
    ")\n",
    "\n",
    "# Set pad_token_id to eos_token_id for GPT-2\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e00f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    r=2,  \n",
    "    lora_alpha=4,  \n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 specific attention modules\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"gpt2-instruction-sft\",\n",
    "    num_train_epochs=10,  # Adjusted for GPT-2\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    max_seq_length=256,  # Appropriate for GPT-2\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d412b",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d7074",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e14887",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('gpt2_instruction_checkpoint')\n",
    "trainer.model.save_pretrained(\"gpt2_instruction_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca8cc3",
   "metadata": {},
   "source": [
    "# 5. Merging Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "BASE_MODEL = \"Savoxism/gpt2-large-continued-pretraining\"  # Your base GPT-2 model\n",
    "ADAPTER_DIR = \"gpt2_instruction_checkpoint\"        \n",
    "MERGED_DIR  = \"gpt2-instruction-merged\"       \n",
    "REPO_ID     = \"Savoxism/gpt2-instruction-finetuned\"  # Update with your repo\n",
    "\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "peft_model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "merged = peft_model.merge_and_unload()\n",
    "\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "merged.save_pretrained(MERGED_DIR)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "\n",
    "print(f\"Merged model saved to {MERGED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hugging Face Hub\n",
    "merged.push_to_hub(REPO_ID, use_auth_token=True)\n",
    "tokenizer.push_to_hub(REPO_ID, use_auth_token=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
