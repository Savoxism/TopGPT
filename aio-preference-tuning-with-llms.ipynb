{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Setup\n\nKeywords: Direct Policy Optimization (DPO), Proximal Policy Optimization (PPO), RLHF, Reward Model\n\nThe general pipeline:\n+ 1. Pre-training: low-quality data + language modeling objective => optimized for text completion\n+ 2. Finetuning: high-quality data + supervised training -> dialogue-like generation\n+ 3. RLHF: comparison data -> scalar score -> reward model ==> Prompt + RL -> Final model","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -U datasets\n!pip install -U trl\n!pip install -U transformers\n!pip install -U accelerate\n!pip install -U bitsandbytes\n!pip install -U sentencepiece\n!pip install -U peft\n!pip install -U huggingface_hub\n!pip install -U warnings\n!pip install -U wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\n# SFT\nfrom trl import SFTConfig, SFTTrainer\nfrom datasets import load_dataset\nimport torch\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig\n\n# DPO\nfrom trl import DPOTrainer, DPOConfig\n\n# diagnostics\nimport warnings\nfrom huggingface_hub import login\nimport wandb\n\n\nCACHE_DIR = \"./cache\"\nBASE_MODEL_ID = \"thainq107/Llama-3.2-1B-Instruct-sft\"\nSFT_OUTPUT_DIR = \"output-SFT\"\nDPO_FULL_OUTPUT_DIR = \"output-DPO-final\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#warning\nwarnings.filterwarnings(\"ignore\")\n\n# huggingface\nAPI_KEY = \"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\"\nlogin(token=API_KEY)\n\n# wandb\nwb_token = \"79126da44d32381139323a9fc5fc6ba0e32b99c4\"\nwandb.login(key=wb_token)\nwandb.init(project=\"Finetuning Llama 3.2 1B Alpaca\", name=\"defaul_run\", reinit=True) # could comment out ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(\"thainq107/Vi-Alpaca-Preference\", cache_dir=CACHE_DIR)\nprint(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = random.randint(1, 60000)\ndataset['train'][idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Supervised Finetuning (SFT)","metadata":{}},{"cell_type":"code","source":"# tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    BASE_MODEL_ID,\n    trust_remote_code=True,\n    cache_dir=CACHE_DIR,\n)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ]\n)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_ID,\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    cache_dir=CACHE_DIR,\n)\nbase_model.config.use_cache = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hyperparameters\nhyperparameters = {\n    \"per_device_train_batch_size\": 32,\n    \"per_device_eval_batch_size\": 8,\n    \"gradient_accumulation_steps\": 2,\n    \"gradient_checkpointing\": True,\n    \"learning_rate\": 3e-5,\n    \"logging_steps\": 500,\n    \"max_steps\": 5000,\n    \"save_strategy\": \"no\",\n    \"overwrite_output_dir\": True,\n    \"optim\": \"paged_adamw_8bit\",\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_steps\": 500,\n    # \"bf16\": True,\n    \"fp16\": True,\n    \"disable_tqdm\": False,\n    \"eval_strategy\": \"steps\",      \n    \"eval_steps\": 500,\n    \"dataloader_num_workers\": 8,\n}\n\nMAX_LENGTH = 512","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# testing \nconversation = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\",   \"content\": \"This is the prompt.\"},\n        {\"role\": \"assistant\",\"content\": \"This is the chosen.\"},\n]\n\nprint(tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_prompt(ex):\n    conv = [\n        {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n        {\"role\":\"user\",\"content\":ex[\"question\"]},\n        {\"role\":\"assistant\",\"content\":ex[\"chosen\"]},\n    ]\n    return tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=False)\n\nsft_config = SFTConfig(\n    **{**hyperparameters, \"output_dir\": SFT_OUTPUT_DIR, \"max_seq_length\": MAX_LENGTH}\n)\n\nsft_trainer = SFTTrainer(\n    model=base_model,\n    peft_config=peft_config,\n    processing_class=tokenizer,\n    args=sft_config,\n    train_dataset=dataset['train'],\n    eval_dataset =dataset['test'],\n    formatting_func=format_prompt,\n)\n\nsft_trainer.train()\nsft_trainer.save_model(\"./output-sft\")\nsft_trainer.push_to_hub(\"Savoxism/Llama-3.2-1B-Instruct-Alpaca-SFT\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Direct Policy Optimization (DPO)","metadata":{}},{"cell_type":"code","source":"# chat template\ndef convert_to_conversational_preference_format(example):\n    return {\n        \"id\": example[\"id\"],\n        \"prompt\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\",   \"content\": example[\"question\"]}\n        ],\n        \"chosen\":  [{\"role\": \"assistant\", \"content\": example[\"chosen\"]}],\n        \"rejected\":[{\"role\": \"assistant\", \"content\": example[\"rejected\"]}],\n    }\n\ndpo_dataset = dataset.map(convert_to_conversational_preference_format)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dpo_full_model = base_model.load_adapter(\n    SFT_OUTPUT_DIR, is_trainable=True, adapter_name=\"dpo_full_adapter\"\n)\n\ndpo_full_args = DPOConfig(\n    **{**hyperparameters, \"output_dir\": DPO_FULL_OUTPUT_DIR, \"max_length\": MAX_LENGTH}\n)\n\ndpo_full_trainer = DPOTrainer(\n    dpo_full_model,\n    args=dpo_full_args,\n    train_dataset=dpo_dataset['train'],\n    eval_dataset =dpo_dataset['test'],\n    processing_class=tokenizer,\n    peft_config=peft_config,\n)\ndpo_full_trainer.train()\ndpo_full_trainer.save_model(DPO_FULL_OUTPUT_DIR)\ndpo_full_trainer.push_to_hub(\"Savoxism/Llama-3.2-1B-Instruct-Alpaca-DPO-full\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Inference & Deployment","metadata":{}},{"cell_type":"code","source":"!pip install -q gradio\n\nimport gradio as gr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# helper functions\ndef get_model_response(model, tokenizer, instruction):\n    cur_conversation = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\",   \"content\": instruction}\n    ]\n    cur_input_prompt = tokenizer.apply_chat_template(\n        cur_conversation, add_generation_prompt=True, tokenize=True\n    )\n    cur_output_ids = model.generate(\n        input_ids=torch.LongTensor([cur_input_prompt]).to(model.device),\n        max_new_tokens=1000\n    )\n    cur_generated_ids = cur_output_ids[0][len(cur_input_prompt):]\n    return tokenizer.decode(cur_generated_ids, skip_special_tokens=True)\n\ndef respond_fn(instruction: str) -> str:\n    return get_model_response(model, tokenizer, instruction)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# interface\niface = gr.Interface(\n    fn=respond_fn,\n    inputs=gr.Textbox(lines=2, placeholder=\"Nhập câu hỏi của bạn...\"),\n    outputs=gr.Textbox(label=\"Phản hồi\"),\n    title=\"Chatbot LLaMA-3.2-1B\",\n    description=\"Giao diện đơn giản cho LLaMA fine-tuned với SFT/DPO\"\n)\niface.launch(share=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}